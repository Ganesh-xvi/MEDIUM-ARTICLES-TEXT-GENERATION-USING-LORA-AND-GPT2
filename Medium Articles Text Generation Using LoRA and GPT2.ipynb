{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5832e28f",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#8A2BE2;margin:0;color:white;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Medium Articles Text Generation Using LoRA and GPT2</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76717d",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <span style=\"padding: 10px; margin: 5px; background-color: #ADD8E6; color: white; font-family: newtimeroman; font-size: 150%; border-radius: 15px; font-weight: 500;\">Project Overview</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43ca8ca",
   "metadata": {},
   "source": [
    "##### The project aims to fine-tune a GPT-2 language model for text generation using LoRA and apply it to generate text based on given prompts. The model is trained on a dataset containing Medium articles, which is cleaned and tokenized before training. The fine-tuning process involves configuring the GPT-2 model with custom settings for LoRA, setting up parameters for training, and evaluating the model's performance using the perplexity score.\n",
    "\n",
    "##### The fine-tuned model is then saved for future use and integrated into a text generation pipeline. This pipeline allows users to input text prompts and receive generated text output based on the trained model's predictions. The project demonstrates a practical application of advanced natural language processing techniques for text generation tasks.\n",
    "\n",
    "##### Overall, the project showcases the process of fine-tuning a language model for specific text generation tasks, highlighting the use of techniques like LoRA to improve the model's performance and the implementation of a text generation pipeline for practical use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a760e4",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <span style=\"padding: 10px; margin: 5px; background-color: #ADD8E6; color: white; font-family: newtimeroman; font-size: 150%; border-radius: 15px; font-weight: 500;\">Import Libraries</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be4e6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkmohankumar1405\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import emoji\n",
    "import torch\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset,DatasetDict\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoConfig, DataCollatorForLanguageModeling, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bef1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model     = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d7a539",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <span style=\"padding: 10px; margin: 5px; background-color: #ADD8E6; color: white; font-family: newtimeroman; font-size: 150%; border-radius: 15px; font-weight: 500;\">Load Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "103a73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Medium Articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f2b5a6",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <span style=\"padding: 10px; margin: 5px; background-color: #ADD8E6; color: white; font-family: newtimeroman; font-size: 150%; border-radius: 15px; font-weight: 500;\">Data Cleaning</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbe1e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_process(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', str(text).lower().strip())\n",
    "    text = re.sub('@[A-Za-z0-9_]+', '', text) \n",
    "    text = re.sub('#','',text) \n",
    "    text = re.sub('RT[\\s]+','',text)\n",
    "    text = re.sub('https?:\\/\\/\\S+', '', text) \n",
    "    text = re.sub('\\n',' ',text)\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "009564cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TexT'] = data['TexT'].apply(cleaning_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67cd1235",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset       = Dataset.from_pandas(data)\n",
    "train_size    = int(len(dataset) * 0.8)\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "test_dataset  = dataset.select(range(train_size, len(dataset)))\n",
    "raw_datasets  = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "981ee475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['TexT'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['TexT'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633c3a1",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <span style=\"padding: 10px; margin: 5px; background-color: #ADD8E6; color: white; font-family: newtimeroman; font-size: 150%; border-radius: 15px; font-weight: 500;\">Convert data into tokenizer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf859e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 128\n",
    "outputs = tokenizer(raw_datasets[\"train\"][:2][\"TexT\"],\n",
    "                    truncation=True,max_length=context_length,\n",
    "                    return_overflowing_tokens=True,return_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4a32ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"TexT\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True)\n",
    "    \n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c0b6ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968c6c025cb4410ba1435c9be08f3c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66866848f524d13931b4b5be1843e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c97a02f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 7718\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1899\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a7005f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"gpt2\",vocab_size=len(tokenizer),n_ctx=context_length,\n",
    "                                     bos_token_id=tokenizer.bos_token_id,\n",
    "                                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3bb7022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d7b5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d187c",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <span style=\"padding: 10px; margin: 5px; background-color: #ADD8E6; color: white; font-family: newtimeroman; font-size: 150%; border-radius: 15px; font-weight: 500;\"> LoRa config </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccde61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a3e9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "255232f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2359296 || all params: 126799104 || trainable%: 1.8606566809809635\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    #target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b7efd6",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <span style=\"padding: 10px; margin: 5px; background-color: #ADD8E6; color: white; font-family: newtimeroman; font-size: 150%; border-radius: 15px; font-weight: 500;\">parameters for training</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d98d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"Medium Article File\",\n",
    "    overwrite_output_dir = True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=500,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=500,\n",
    "    report_to=\"wandb\",\n",
    "    metric_for_best_model = 'accuracy',\n",
    "    run_name = 'custom_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af3a21d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_generated_text(generated_text):\n",
    "    if isinstance(generated_text, dict):\n",
    "\n",
    "        text = generated_text.get('text', '')\n",
    "        \n",
    "        formatted_text = text.strip()\n",
    "        \n",
    "        formatted_text = formatted_text.capitalize()\n",
    "        \n",
    "        if not formatted_text.endswith('.'):\n",
    "            formatted_text += '.'\n",
    "        return formatted_text\n",
    "    else:\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcefb23a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02930a897c464b4f9d8191650e346889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c6d8015a0b483eb6fc59854beeecb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    packing=True,\n",
    "    formatting_func=format_generated_text,\n",
    "    peft_config=peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daacfbe3",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <span style=\"padding: 10px; margin: 5px; background-color: #ADD8E6; color: white; font-family: newtimeroman; font-size: 150%; border-radius: 15px; font-weight: 500;\">Fine Tuning Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5a343ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\kumar\\Data science\\9.GEN AI\\3.Gen AI Class Workouts\\New folder\\wandb\\run-20240218_232710-82ujr11c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kmohankumar1405/Medium%20Article%20Generation/runs/82ujr11c' target=\"_blank\">twinkling-envelope-1</a></strong> to <a href='https://wandb.ai/kmohankumar1405/Medium%20Article%20Generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kmohankumar1405/Medium%20Article%20Generation' target=\"_blank\">https://wandb.ai/kmohankumar1405/Medium%20Article%20Generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kmohankumar1405/Medium%20Article%20Generation/runs/82ujr11c' target=\"_blank\">https://wandb.ai/kmohankumar1405/Medium%20Article%20Generation/runs/82ujr11c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kmohankumar1405/Medium%20Article%20Generation/runs/82ujr11c?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x23603656680>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Medium Article Generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5b5cedf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 22:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=10.24756088256836, metrics={'train_runtime': 1485.5462, 'train_samples_per_second': 0.05, 'train_steps_per_second': 0.007, 'total_flos': 40280968396800.0, 'train_loss': 10.24756088256836, 'epoch': 5.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1baf3f",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <span style=\"padding: 10px; margin: 5px; background-color: #ADD8E6; color: white; font-family: newtimeroman; font-size: 150%; border-radius: 15px; font-weight: 500;\"> Perplexity Score check</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f4799d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 8764.61\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207f8497",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <span style=\"padding: 10px; margin: 5px; background-color: #ADD8E6; color: white; font-family: newtimeroman; font-size: 150%; border-radius: 15px; font-weight: 500;\">wandb score</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f113f568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e77b8694a24fc693b3096f16c4b788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>9.07848</td></tr><tr><td>eval/runtime</td><td>16.7935</td></tr><tr><td>eval/samples_per_second</td><td>0.179</td></tr><tr><td>eval/steps_per_second</td><td>0.06</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>10</td></tr><tr><td>train/total_flos</td><td>40280968396800.0</td></tr><tr><td>train/train_loss</td><td>10.24756</td></tr><tr><td>train/train_runtime</td><td>1485.5462</td></tr><tr><td>train/train_samples_per_second</td><td>0.05</td></tr><tr><td>train/train_steps_per_second</td><td>0.007</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twinkling-envelope-1</strong> at: <a href='https://wandb.ai/kmohankumar1405/Medium%20Article%20Generation/runs/82ujr11c' target=\"_blank\">https://wandb.ai/kmohankumar1405/Medium%20Article%20Generation/runs/82ujr11c</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240218_232710-82ujr11c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f3898b",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <span style=\"padding: 10px; margin: 5px; background-color: #ADD8E6; color: white; font-family: newtimeroman; font-size: 150%; border-radius: 15px; font-weight: 500;\">Saved a Fine Tuned Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae6161aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8be317ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'Medium Article File'\n",
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3861d",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <span style=\"padding: 10px; margin: 5px; background-color: #ADD8E6; color: white; font-family: newtimeroman; font-size: 150%; border-radius: 15px; font-weight: 500;\">Model into Pipeline</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36ea07b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "output_test = pipeline('text-generation',model='Medium Article File', tokenizer='gpt2')\n",
    "\n",
    "#result = chef('NLP is Good thing to have in future')[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6543fe68",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <span style=\"padding: 10px; margin: 5px; background-color: #ADD8E6; color: white; font-family: newtimeroman; font-size: 150%; border-radius: 15px; font-weight: 500;\">Test the Fine Tuned Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab4df76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Neural Networks/E-PFC\\n\\n\\nIn some areas where there is a difference in brain structure-we have a similar type of brain for language. However, it is most often difficult to pinpoint with our own eyes where those differences arise.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test('Neural Networks')[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbf49002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Data science for your business, not just for you. You shouldn't rely exclusively on your computer science background to make a living.\\n\\nTo do so, first make sure your project is successful and do your best to answer every question on the site\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test('Data science')[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42f43806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'what is a codeing function)?\\n\\nAnswer: I like to use the Python interpreter on my Mac. In this test, I choose a variable that contains the path to that variable:\\n\\nimport os def b = os.path / obj'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test('what is a codeing')[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbf3915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"GPU is very Low Profile as opposed to an R&D system, and the fact it does not utilize a much more powerful CPU at all, will ensure that the price of its predecessor really doesn't drop to the stratosphere.\\n\\nIt was\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test('GPU is very Low')[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c2618",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <span style=\"padding: 10px; margin: 5px; background-color: #ADD8E6; color: white; font-family: newtimeroman; font-size: 150%; border-radius: 15px; font-weight: 500;\">Conclusion</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee1273",
   "metadata": {},
   "source": [
    "##### This project showcases the effective fine-tuning of a GPT-2 language model using LoRA for text generation tasks. The model is trained on a dataset of Medium articles, demonstrating the importance of data preprocessing and tokenization for training. The fine-tuned model achieves a perplexity score of approximately 8764.61 on the test dataset, indicating its ability to predict the next word in a sequence.\n",
    "\n",
    "##### The project highlights the practical applications of the fine-tuned model, including content generation, chatbot development, text summarization, and creative writing assistance. It also emphasizes the model's potential for content personalization and language translation tasks.\n",
    "\n",
    "##### Overall, this project demonstrates the versatility and effectiveness of fine-tuning transformer-based models like GPT-2 for a variety of natural language processing tasks, paving the way for more advanced and context-aware text generation applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
